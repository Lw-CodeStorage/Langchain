{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "conn = \"mysql+pymysql://root:123@127.0.0.1/sys\"\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次对话后的记忆: Human: 我姐姐明天要过生日，我需要一束生日花束。\n",
      "AI: 生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\n",
      "\n",
      "另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。 \n",
      "\n",
      "第二次对话后的记忆: Human: 我姐姐明天要过生日，我需要一束生日花束。\n",
      "AI: 生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\n",
      "\n",
      "另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。\n",
      "Human: 她喜欢粉色玫瑰，颜色是粉色的。\n",
      "AI: 那就很好！粉色玫瑰是一种非常浪漫且温馨的选择。根据你的描述，我可以为你姐姐准备一束粉色玫瑰花束。\n",
      "\n",
      "我可以提供一些设计方案给你参考。例如，“爱心之花”通常包括5-7朵粉色玫瑰花，配以一些绿叶和丝带，以表达对姐姐的深厚爱意。如果你想知道更多关于这个设计方案的信息，请告诉我。\n",
      "\n",
      "另外，我还可以提供一些其他选项，如添加一些白色或淡黄色的玫瑰花来增强花束的视觉效果。或者，如果你想给姐姐一个特别的惊喜，我们也可以考虑添加一些特殊的装饰品，例如小心形状的纸盒、丝带等。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是明天，对吗？ \n",
      "\n",
      "\n",
      "第三次对话后时提示:\n",
      " The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n",
      "\n",
      "第三次对话后的记忆:\n",
      " Human: 我姐姐明天要过生日，我需要一束生日花束。\n",
      "AI: 生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\n",
      "\n",
      "另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。\n",
      "Human: 她喜欢粉色玫瑰，颜色是粉色的。\n",
      "AI: 那就很好！粉色玫瑰是一种非常浪漫且温馨的选择。根据你的描述，我可以为你姐姐准备一束粉色玫瑰花束。\n",
      "\n",
      "我可以提供一些设计方案给你参考。例如，“爱心之花”通常包括5-7朵粉色玫瑰花，配以一些绿叶和丝带，以表达对姐姐的深厚爱意。如果你想知道更多关于这个设计方案的信息，请告诉我。\n",
      "\n",
      "另外，我还可以提供一些其他选项，如添加一些白色或淡黄色的玫瑰花来增强花束的视觉效果。或者，如果你想给姐姐一个特别的惊喜，我们也可以考虑添加一些特殊的装饰品，例如小心形状的纸盒、丝带等。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是明天，对吗？\n",
      "Human: 我又来了，还记得我昨天为什么要来买花吗？\n",
      "AI: 你昨天来买花是因为你的姐姐明天要过生日！我记得很清楚。我们讨论了关于选择粉色玫瑰、设计方案和添加其他装饰品的选项。现在，我们已经确定了一个基本的花束方案，包括5-7朵粉色玫瑰花。还有什么需要调整或添加的吗？ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "#第一天的对话\n",
    "#回合1\n",
    "conversation(\"我姐姐明天要过生日，我需要一束生日花束。\")\n",
    "print(\"第一次对话后的记忆:\", conversation.memory.buffer,\"\\n\")\n",
    "\n",
    "# 回合2\n",
    "conversation(\"她喜欢粉色玫瑰，颜色是粉色的。\")\n",
    "print(\"第二次对话后的记忆:\", conversation.memory.buffer,\"\\n\")\n",
    "\n",
    "# 回合3 （第二天的对话）\n",
    "conversation(\"我又来了，还记得我昨天为什么要来买花吗？\")\n",
    "print(\"\\n第三次对话后时提示:\\n\",conversation.prompt.template)\n",
    "print(\"\\n第三次对话后的记忆:\\n\", conversation.memory.buffer,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='我姐姐明天要过生日，我需要一束生日花束。', additional_kwargs={}, response_metadata={}), AIMessage(content='生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\\n\\n另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\\n\\n最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。', additional_kwargs={}, response_metadata={}), HumanMessage(content='她喜欢粉色玫瑰，颜色是粉色的。', additional_kwargs={}, response_metadata={}), AIMessage(content='那就很好！粉色玫瑰是一种非常浪漫且温馨的选择。根据你的描述，我可以为你姐姐准备一束粉色玫瑰花束。\\n\\n我可以提供一些设计方案给你参考。例如，“爱心之花”通常包括5-7朵粉色玫瑰花，配以一些绿叶和丝带，以表达对姐姐的深厚爱意。如果你想知道更多关于这个设计方案的信息，请告诉我。\\n\\n另外，我还可以提供一些其他选项，如添加一些白色或淡黄色的玫瑰花来增强花束的视觉效果。或者，如果你想给姐姐一个特别的惊喜，我们也可以考虑添加一些特殊的装饰品，例如小心形状的纸盒、丝带等。\\n\\n最后，我需要确认一下，你姐姐的生日是明天，对吗？', additional_kwargs={}, response_metadata={}), HumanMessage(content='我又来了，还记得我昨天为什么要来买花吗？', additional_kwargs={}, response_metadata={}), AIMessage(content='你昨天来买花是因为你的姐姐明天要过生日！我记得很清楚。我们讨论了关于选择粉色玫瑰、设计方案和添加其他装饰品的选项。现在，我们已经确定了一个基本的花束方案，包括5-7朵粉色玫瑰花。还有什么需要调整或添加的吗？', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Pierce. We just established that!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# store 存储所有对话历史，作为memory\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"test001\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Pierce\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test001': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm Pierce\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Nice to meet you, Pierce! How's your day going so far? Is there something on your mind that you'd like to chat about or is this just a friendly hello?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-10-10T17:24:54.105258636Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4338289776, 'load_duration': 10010420, 'prompt_eval_count': 15, 'prompt_eval_duration': 121696000, 'eval_count': 37, 'eval_duration': 4146556000}, id='run-22c15381-a971-4bfa-982c-98ee839b821b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 37, 'total_tokens': 52}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Pierce. We just established that!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-10-10T17:24:56.127087917Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2019049818, 'load_duration': 57871170, 'prompt_eval_count': 66, 'prompt_eval_duration': 651766000, 'eval_count': 11, 'eval_duration': 1158381000}, id='run-82a8c139-a854-463b-9aee-962fe8c3417e-0', usage_metadata={'input_tokens': 66, 'output_tokens': 11, 'total_tokens': 77})])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "openai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
