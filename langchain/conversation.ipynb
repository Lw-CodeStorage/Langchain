{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "conn = \"mysql+pymysql://root:123@127.0.0.1/sys\"\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次对话后的记忆: Human: 我姐姐明天要过生日，我需要一束生日花束。\n",
      "AI: 生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\n",
      "\n",
      "另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。 \n",
      "\n",
      "第二次对话后的记忆: Human: 我姐姐明天要过生日，我需要一束生日花束。\n",
      "AI: 生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\n",
      "\n",
      "另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。\n",
      "Human: 她喜欢粉色玫瑰，颜色是粉色的。\n",
      "AI: 那就很好！粉色玫瑰是一种非常浪漫且温馨的选择。根据你的描述，我可以为你姐姐准备一束粉色玫瑰花束。\n",
      "\n",
      "我可以提供一些设计方案给你参考。例如，“爱心之花”通常包括5-7朵粉色玫瑰花，配以一些绿叶和丝带，以表达对姐姐的深厚爱意。如果你想知道更多关于这个设计方案的信息，请告诉我。\n",
      "\n",
      "另外，我还可以提供一些其他选项，如添加一些白色或淡黄色的玫瑰花来增强花束的视觉效果。或者，如果你想给姐姐一个特别的惊喜，我们也可以考虑添加一些特殊的装饰品，例如小心形状的纸盒、丝带等。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是明天，对吗？ \n",
      "\n",
      "\n",
      "第三次对话后时提示:\n",
      " The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n",
      "\n",
      "第三次对话后的记忆:\n",
      " Human: 我姐姐明天要过生日，我需要一束生日花束。\n",
      "AI: 生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\n",
      "\n",
      "另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。\n",
      "Human: 她喜欢粉色玫瑰，颜色是粉色的。\n",
      "AI: 那就很好！粉色玫瑰是一种非常浪漫且温馨的选择。根据你的描述，我可以为你姐姐准备一束粉色玫瑰花束。\n",
      "\n",
      "我可以提供一些设计方案给你参考。例如，“爱心之花”通常包括5-7朵粉色玫瑰花，配以一些绿叶和丝带，以表达对姐姐的深厚爱意。如果你想知道更多关于这个设计方案的信息，请告诉我。\n",
      "\n",
      "另外，我还可以提供一些其他选项，如添加一些白色或淡黄色的玫瑰花来增强花束的视觉效果。或者，如果你想给姐姐一个特别的惊喜，我们也可以考虑添加一些特殊的装饰品，例如小心形状的纸盒、丝带等。\n",
      "\n",
      "最后，我需要确认一下，你姐姐的生日是明天，对吗？\n",
      "Human: 我又来了，还记得我昨天为什么要来买花吗？\n",
      "AI: 你昨天来买花是因为你的姐姐明天要过生日！我记得很清楚。我们讨论了关于选择粉色玫瑰、设计方案和添加其他装饰品的选项。现在，我们已经确定了一个基本的花束方案，包括5-7朵粉色玫瑰花。还有什么需要调整或添加的吗？ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "#第一天的对话\n",
    "#回合1\n",
    "conversation(\"我姐姐明天要过生日，我需要一束生日花束。\")\n",
    "print(\"第一次对话后的记忆:\", conversation.memory.buffer,\"\\n\")\n",
    "\n",
    "# 回合2\n",
    "conversation(\"她喜欢粉色玫瑰，颜色是粉色的。\")\n",
    "print(\"第二次对话后的记忆:\", conversation.memory.buffer,\"\\n\")\n",
    "\n",
    "# 回合3 （第二天的对话）\n",
    "conversation(\"我又来了，还记得我昨天为什么要来买花吗？\")\n",
    "print(\"\\n第三次对话后时提示:\\n\",conversation.prompt.template)\n",
    "print(\"\\n第三次对话后的记忆:\\n\", conversation.memory.buffer,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='我姐姐明天要过生日，我需要一束生日花束。', additional_kwargs={}, response_metadata={}), AIMessage(content='生日花束！我可以提供一些帮助。根据你所在的地区和喜好，你可以选择各种类型的鲜花来组成生日花束。例如，如果你姐姐喜欢玫瑰，那么我们可以选择一些红、粉色或白色的玫瑰花来组成花束。如果你姐姐更喜欢其他类型的花，如康乃馨、百合或者菊花，我们也可以提供相应的选项。\\n\\n另外，我还可以提供一些生日花束的设计方案，例如“爱心之花”、“幸福之花”等。这些设计方案通常包括特定的花色和数量组合，以表达对姐姐的祝福。如果你想知道更多关于这些设计方案的信息，请告诉我。\\n\\n最后，我需要确认一下，你姐姐的生日是哪一天？这样我才能准确地提供相关的信息和建议。', additional_kwargs={}, response_metadata={}), HumanMessage(content='她喜欢粉色玫瑰，颜色是粉色的。', additional_kwargs={}, response_metadata={}), AIMessage(content='那就很好！粉色玫瑰是一种非常浪漫且温馨的选择。根据你的描述，我可以为你姐姐准备一束粉色玫瑰花束。\\n\\n我可以提供一些设计方案给你参考。例如，“爱心之花”通常包括5-7朵粉色玫瑰花，配以一些绿叶和丝带，以表达对姐姐的深厚爱意。如果你想知道更多关于这个设计方案的信息，请告诉我。\\n\\n另外，我还可以提供一些其他选项，如添加一些白色或淡黄色的玫瑰花来增强花束的视觉效果。或者，如果你想给姐姐一个特别的惊喜，我们也可以考虑添加一些特殊的装饰品，例如小心形状的纸盒、丝带等。\\n\\n最后，我需要确认一下，你姐姐的生日是明天，对吗？', additional_kwargs={}, response_metadata={}), HumanMessage(content='我又来了，还记得我昨天为什么要来买花吗？', additional_kwargs={}, response_metadata={}), AIMessage(content='你昨天来买花是因为你的姐姐明天要过生日！我记得很清楚。我们讨论了关于选择粉色玫瑰、设计方案和添加其他装饰品的选项。现在，我们已经确定了一个基本的花束方案，包括5-7朵粉色玫瑰花。还有什么需要调整或添加的吗？', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_43668800/article/details/140084487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Pierce. We just established that!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# store 存储所有对话历史，作为memory\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"test001\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Pierce\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test001': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm Pierce\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Nice to meet you, Pierce! How's your day going so far? Is there something on your mind that you'd like to chat about or is this just a friendly hello?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-10-10T18:00:05.659256483Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 5019220922, 'load_duration': 10605747, 'prompt_eval_count': 15, 'prompt_eval_duration': 551099000, 'eval_count': 37, 'eval_duration': 4405714000}, id='run-7c4e36c0-9f9c-491c-8e9c-b732267151aa-0', usage_metadata={'input_tokens': 15, 'output_tokens': 37, 'total_tokens': 52}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Pierce. We just established that!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-10-10T18:00:07.947174181Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2285207728, 'load_duration': 60023529, 'prompt_eval_count': 66, 'prompt_eval_duration': 789697000, 'eval_count': 11, 'eval_duration': 1274481000}, id='run-1cfb23fc-5ffb-4976-9d10-e9103ae4912f-0', usage_metadata={'input_tokens': 66, 'output_tokens': 11, 'total_tokens': 77})])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages, AIMessage\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(...)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "openai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
