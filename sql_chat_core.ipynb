{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "conn = \"mysql+pymysql://root:123@127.0.0.1/sys\"\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "llm.predict(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "db = SQLDatabase.from_uri(\n",
    "    \"mysql+pymysql://root:123@127.0.0.1/chinook\", \n",
    "    include_tables=[\"employee\"],\n",
    "    sample_rows_in_table_info=3\n",
    "    )\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Please generate only an executable SQL query, strictly following the structure and using the schema below. Do not include explanations or additional text.\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "def get_schema(_):\n",
    "    schema = db.get_table_info()\n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | llm\n",
    "    # | llm.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sql_chain.invoke({\"question\": \"總共有多少資料\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\n",
    "\"\"\"\n",
    "\n",
    "# template = \"\"\"\n",
    "# \"Based on the query results, and aiming to compile all available data as much as possible, \n",
    "# respond in one of the preferred formats: Highcharts.js or a bullet-point summary of the results. \n",
    "# If the data involves trends, statistics, or analysis that can be visually represented, \n",
    "# prioritize determining whether a Highcharts.js visualization can be created, \n",
    "# and only return the Highcharts.chart code. Any other information is unnecessary,\n",
    "# and no explanation is needed for cases where a chart cannot be generated.\":\n",
    "# {schema}\n",
    "\n",
    "# Question: {question}\n",
    "# SQL Query: {query}\n",
    "# SQL Response: {response}\n",
    "# \"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def run_query(query):\n",
    "    return db.run(query)\n",
    "\n",
    "main_chain = (\n",
    "    RunnablePassthrough\n",
    "    .assign(query=sql_chain) # 上一個chain的結果\n",
    "    .assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x : run_query(x[\"query\"]) # sql 執行的結果\n",
    "    )\n",
    "    | prompt_response\n",
    "    | llm\n",
    ")\n",
    "main_chain.invoke({\"question\":\"有多少資料\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "串列處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "並列處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def add_two(x: int) -> int:\n",
    "    return x + 2\n",
    "\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(add_two)\n",
    "\n",
    "parallel = {\"runnable_1\": runnable_1, \"runnable_2\": runnable_2}\n",
    "\n",
    "chain = RunnableLambda(lambda x: x) | parallel\n",
    "answer = chain.invoke(1)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "summ_chain = (\n",
    "    ChatPromptTemplate.from_template(\"幫我依據資料總結資訊，以{type}呈現\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "char_chain = (\n",
    "    ChatPromptTemplate.from_template(\"依據資料生成可繪製的highchart.js格式，不需要其他資訊\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "main =  RunnableParallel(joke=summ_chain, poem=char_chain)\n",
    "\n",
    "main.invoke({\"type\":\"條列式\"}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "conn = \"mysql+pymysql://root:123@127.0.0.1/sys\"\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "db = SQLDatabase.from_uri(\n",
    "    \"mysql+pymysql://root:123@127.0.0.1/chinook\", \n",
    "    include_tables=[\"employee\"],\n",
    "    sample_rows_in_table_info=3\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(*) FROM employee;\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Please generate only an executable SQL query, strictly following the structure and using the schema below. Do not include explanations or additional text.\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "def get_schema(_):\n",
    "    schema = db.get_table_info()\n",
    "    return schema\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sql = sql_chain.invoke({\"question\":\"有多少資料\"})\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='有 8 個員工的資料。', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-09-29T13:57:29.632042244Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 25814255383, 'load_duration': 13407892, 'prompt_eval_count': 767, 'prompt_eval_duration': 24635190000, 'eval_count': 10, 'eval_duration': 1111316000}, id='run-a3b8b3a3-bc55-448a-aad5-af057fe99ada-0', usage_metadata={'input_tokens': 767, 'output_tokens': 10, 'total_tokens': 777})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_response = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def run_query(query):\n",
    "    return db.run(query)\n",
    "\n",
    "main_chain = (\n",
    "    RunnablePassthrough\n",
    "    .assign(query=lambda x : sql)\n",
    "    .assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x : run_query(x[\"query\"])\n",
    "    )\n",
    "    | prompt_response\n",
    "    | llm\n",
    ")\n",
    "\n",
    "main_chain.invoke({\"question\":\"有多少資料\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 +----------------------+                    \n",
      "                 | Parallel<query>Input |                    \n",
      "                 +----------------------+                    \n",
      "                       **         ***                        \n",
      "                     **              *                       \n",
      "                    *                 **                     \n",
      "             +--------+          +-------------+             \n",
      "             | Lambda |          | Passthrough |             \n",
      "             +--------+          +-------------+             \n",
      "                       **         ***                        \n",
      "                         **      *                           \n",
      "                           *   **                            \n",
      "                 +-----------------------+                   \n",
      "                 | Parallel<query>Output |                   \n",
      "                 +-----------------------+                   \n",
      "                             *                               \n",
      "                             *                               \n",
      "                             *                               \n",
      "            +--------------------------------+               \n",
      "            | Parallel<schema,response>Input |               \n",
      "            +--------------------------------+               \n",
      "                 ****        *         ****                  \n",
      "             ****            *             ****              \n",
      "           **                *                 **            \n",
      "+------------+          +--------+          +-------------+  \n",
      "| get_schema |          | Lambda |          | Passthrough |  \n",
      "+------------+***       +--------+         *+-------------+  \n",
      "                 ****        *         ****                  \n",
      "                     ****    *     ****                      \n",
      "                         **  *   **                          \n",
      "            +---------------------------------+              \n",
      "            | Parallel<schema,response>Output |              \n",
      "            +---------------------------------+              \n",
      "                             *                               \n",
      "                             *                               \n",
      "                             *                               \n",
      "                  +--------------------+                     \n",
      "                  | ChatPromptTemplate |                     \n",
      "                  +--------------------+                     \n",
      "                             *                               \n",
      "                             *                               \n",
      "                             *                               \n",
      "                      +------------+                         \n",
      "                      | ChatOllama |                         \n",
      "                      +------------+                         \n",
      "                             *                               \n",
      "                             *                               \n",
      "                             *                               \n",
      "                   +------------------+                      \n",
      "                   | ChatOllamaOutput |                      \n",
      "                   +------------------+                      \n"
     ]
    }
   ],
   "source": [
    "main_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         +-----------------------+           \n",
      "         | Parallel<schema>Input |           \n",
      "         +-----------------------+           \n",
      "             ***            **               \n",
      "           **                 **             \n",
      "         **                     **           \n",
      "+------------+              +-------------+  \n",
      "| get_schema |              | Passthrough |  \n",
      "+------------+              +-------------+  \n",
      "             ***            **               \n",
      "                **        **                 \n",
      "                  **    **                   \n",
      "        +------------------------+           \n",
      "        | Parallel<schema>Output |           \n",
      "        +------------------------+           \n",
      "                     *                       \n",
      "                     *                       \n",
      "                     *                       \n",
      "          +--------------------+             \n",
      "          | ChatPromptTemplate |             \n",
      "          +--------------------+             \n",
      "                     *                       \n",
      "                     *                       \n",
      "                     *                       \n",
      "              +------------+                 \n",
      "              | ChatOllama |                 \n",
      "              +------------+                 \n",
      "                     *                       \n",
      "                     *                       \n",
      "                     *                       \n",
      "            +-----------------+              \n",
      "            | StrOutputParser |              \n",
      "            +-----------------+              \n",
      "                     *                       \n",
      "                     *                       \n",
      "                     *                       \n",
      "         +-----------------------+           \n",
      "         | StrOutputParserOutput |           \n",
      "         +-----------------------+           \n"
     ]
    }
   ],
   "source": [
    "sql_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "openai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
